{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Members\n",
    "\n",
    "**Ojo, Oluwaseun** | **Sholola, Oluwafunmiwo Judah**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context of Dataset and Goal Overview\n",
    "\n",
    "For our dataset, we settled on automotive data from MarketCheck spanning 8 years of inventory in Canada. Each record indicates the most recent online activity for a particular vehicle, obtained via webcrawlers that aggregate info retrieved from a vast number (~65k) of dealer websites. Online activity is not defined, but it is reasonably safe (based on the 'price' attribute provided) to assume that the records indicate sale listings (and not leases, or rentals).\n",
    "\n",
    "According to the dataset's [Kaggle link](https://www.kaggle.com/datasets/3ea0a6a45dbd4713a8759988845f1a58038036d84515ded58f65a2ff2bd32e00?resource=download), it was last updated a year ago, so we can reasonably expect 2021 to be the upper limit for the year of car manufacture.\n",
    "\n",
    "This dataset comprises some expected attributes such as the price, make, mileage and manufacturing year of the car. It also includes more specific (and potentially more interesting) attributes such as the fuel type, the engine size, the (corporate) dealer name and site of activity (zip and province).\n",
    "\n",
    "While our exact goal is not settled upon just yet, as we have not studied the data in-depth enough to hone in on a single phenom, listed below are a few possible phenomenon that are in contention to be studied by us in future iterations:\n",
    "\n",
    "* correlation of price to the make and year of a vehicle\n",
    "* correlation of vehicle fuel type to sales (or frequency of sales, more specifically)\n",
    "* the ideal car (or kind of car) to buy based on the age of vehicles\n",
    "* the ideal province to buy a used car within Canada taking price and mileage into consideration\n",
    "\n",
    "That said, let us dive in!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to importing our dataset, we quickly observe that the total number of records in the dataset and the number of unique VIN values **DO NOT** match. \n",
    "\n",
    "The reason for this disparity in the number of records and the VIN's is due to the fact that certain \n",
    "cars come up for sale multiple times within the collection window of the dataset (the past year), or they come multiple different dealer sites.\n",
    "\n",
    "<img src=\"img\\kaggle-id-vin.png\" width=\"600\" height=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fshol\\AppData\\Local\\Temp\\ipykernel_10120\\2962496088.py:6: DtypeWarning: Columns (13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('ca-dealers-used.csv').sort_index()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count                393603\n",
       "unique               226691\n",
       "top       5LMJJ2JT1KEL20643\n",
       "freq                     21\n",
       "Name: vin, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "df = pd.read_csv('ca-dealers-used.csv').sort_index()\n",
    "\n",
    "# confirming what we observe at a glance on Kaggle\n",
    "df['vin'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index selection\n",
    "\n",
    "Logically, it makes sense to use either the **id** attribute (which is unique) or the **vin** attribute (which, ideally, would be unique in the dataset).\n",
    "\n",
    "Since some VIN's pop up a few times in the dataset as a result of certain vehicles being traded multiple times, we have to decide:\n",
    "* if we want to drop duplicate VIN's and simply represent each vehicle once throughout the dataframe, thus making the **vin** attribute unique and possible for use as the **index** of the dataframe; *or*\n",
    "* if we want to keep multiple instances of a vehicle in the dataframe and utilize **id** as a single index, or **(id, vin)** as a MultiIndex **index** of the dataframe\n",
    "\n",
    "We decided to drop rows with duplicate VIN's as we do not require multiple instances of a vehicle for the purposes of our analysis. Subsequently, we set **id** attribute as our index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with duplicate VIN and set 'id' as our index\n",
    "df_drop = df.copy()\n",
    "df_drop = df_drop.drop_duplicates('vin')\n",
    "df_drop = df_drop.reset_index(drop=True).set_index('id')\n",
    "\n",
    "display(df_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a basic, high-level overview\n",
    "display(df_drop.info())\n",
    "\n",
    "# general description of numerical attributes in dataset\n",
    "display(df_drop.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for dropping certain columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision to drop these columns is moreso based on some domain knowledge and logic.\n",
    "\n",
    "The **stock_no** attribute is unique to each dealer's online inventory categorization system, and is not of a singular format; some are purely numeric in nature, while some are alphanumeric. As such, it is not extremely useful across diverse sources (dealers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first 20 'stock_no' fields\n",
    "display(df_drop['stock_no'].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to drop **trim** attribute  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_drop.groupby('trim')['make'].describe())\n",
    "display(sum(df_drop.groupby('trim')['make'].describe()['unique']))\n",
    "\n",
    "# sum(df_drop.groupby('make')['trim'].describe()['unique'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **engine_block** is a low-level attribute that has **3** possible values:\n",
    "* H-engine type\n",
    "* I-engine type\n",
    "* V-engine type\n",
    "\n",
    "It is not a particularly desirable attribute for our purposes. \n",
    "\n",
    "Also, unlike some other attributes such as the manufacturing **year**, which can be decoded from the **VIN** and are uniform/standard across various vehicle manufacturers, the **engine_block** code in the VIN differs across manufacturers (**make**) and even across **models** within the same make. \n",
    "\n",
    "As such, it rapidly devolves into an arduous task in trying to retrieve an attribute that we, once again, do not view as useful to our goal(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_drop.groupby('engine_block').count())\n",
    "\n",
    "df_drop['engine_block'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **street** and **zip** attributes are being dropped in favour of 2 other geo-based attributes, **city** and **state**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proceeding to drop the unwanted columns and reordering our retained columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the columns and drop them\n",
    "col_to_drop = ['stock_no', 'trim', 'engine_block', 'street', 'zip']\n",
    "df_drop = df_drop.drop(columns=col_to_drop)\n",
    "\n",
    "# specify an order of columns and order them accordingly\n",
    "order_of_cols = ['vin', 'make', 'model', 'year', 'age','miles', 'price', 'fuel_type', 'engine_size', 'body_type',  'vehicle_type', 'transmission', 'drivetrain', 'seller_name', 'city', 'state']\n",
    "df_drop = df_drop[order_of_cols]\n",
    "\n",
    "# rename state to province (more appropriate to Canada)\n",
    "df_drop.rename(columns={'state' : 'province'}, inplace=True)\n",
    "\n",
    "df_drop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusion of an Additional (Derived) Column\n",
    "\n",
    "We decided to derive a new column, **age**, based off the manufacturing **year**, which we use to prune off vehicles that fall outside a specific range. It is important to note that this is not a trimming off of outliers, rather we are choosing deal *only* with cars that were manufactured *more recently*.\n",
    "\n",
    "This attribute might also come in handy in extracting useful features, such as the relationship between the manufacturing **year** of vehicles and the **frequency** (number) of sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add age col generation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retained Columns and their Descriptions (contextualized)\n",
    "\n",
    "**id** : unique identifier for each row\n",
    "\n",
    "**vin** : 17 character long vehicle identification number. This is the value that uniquely identifies each vehicle.\n",
    "\n",
    "**make** : Make of the vehicle - Decoded from VIN. This is the manufacturer of the vehicle.\n",
    "\n",
    "**model** : Model of the vehicle - Decoded from VIN. It is the specific model of the vehicle.\n",
    "\n",
    "**year** : Model year of the vehicle - Decoded from VIN. This is the year the vehicle was manufactured.\n",
    "\n",
    "**age** : How recently the car was manufactured, using 2022 as a reference point.\n",
    "\n",
    "**miles** : Number of miles/odometer on the vehicle. The higher the number, the more the vehicle has been driven or used.\n",
    "\n",
    "**price** : Price of the vehicle. This is the value that we will try to predict.\n",
    "\n",
    "**fuel_type** : This is the type of fuel the vehicle uses. For example, gasoline, diesel, etc.\n",
    "\n",
    "**engine_size** : This is the size of the engine in liters.\n",
    "\n",
    "**body_type** : Body type of the vehicle - Decoded from VIN. This is the type of vehicle. For example, sedan, coupe, SUV, etc.\n",
    "\n",
    "**vehicle_type** : This is the type of vehicle. For example, car, truck, etc.\n",
    "\n",
    "**transmission** : Transmission type of the vehicle. The transmission type is either automatic or manual.\n",
    "\n",
    "**drivetrain** : Drivetrain type of the vehicle. The drivetrain type is either 4WD, FWD, or RWD.\n",
    "\n",
    "**seller_name** : Name of the seller or dealership\n",
    "\n",
    "**city** : City of the seller or dealership.\n",
    "\n",
    "**province** : Province of the seller or dealership\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in Missing Data\n",
    "Two attributes we decided on, for addressing missing data are:\n",
    "* **year**; *and*\n",
    "* **engine_size**\n",
    "\n",
    "For **year**, it is **not** missing from the original dataset, so we are going to artificially introduce missing values in some rows. The intention (and result) is to repoduce these values by decoding the VIN. It is a standard across all manufacturers that the **10th digit** in the VIN is a *letter* or a *number*, which corresponds to the manufacturing year of the vehicle.\n",
    "\n",
    "<img src=\"img\\vin-year-dict-mapping.png\" width=\"600\" height=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    " # add filling year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **engine_size**, about **18%** of values are missing from this attribute in the original dataset. The engine size can be derived from the VIN, but the encoding is unique to each manufacturer and also to the particular model of a vehicle, so using a dictionary would be expensive (memory-wise). As such, we utilize centrality measures, based on categorization fo a related column (**body_type**), to fill missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_drop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Users\\ojS\\Documents\\Family\\Miwo\\Graduate Studies\\MemorialU\\Pre-Adm. & Admission Documents\\Courses\\Terms\\Fall 2022\\COMP 6981 (DataPrepTech)\\proj\\COMP6981-Project-Iteration1\\Project_Iteration_1.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Users/ojS/Documents/Family/Miwo/Graduate%20Studies/MemorialU/Pre-Adm.%20%26%20Admission%20Documents/Courses/Terms/Fall%202022/COMP%206981%20%28DataPrepTech%29/proj/COMP6981-Project-Iteration1/Project_Iteration_1.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_temp \u001b[39m=\u001b[39m df_drop\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/ojS/Documents/Family/Miwo/Graduate%20Studies/MemorialU/Pre-Adm.%20%26%20Admission%20Documents/Courses/Terms/Fall%202022/COMP%206981%20%28DataPrepTech%29/proj/COMP6981-Project-Iteration1/Project_Iteration_1.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# engine_size per body_type category\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/ojS/Documents/Family/Miwo/Graduate%20Studies/MemorialU/Pre-Adm.%20%26%20Admission%20Documents/Courses/Terms/Fall%202022/COMP%206981%20%28DataPrepTech%29/proj/COMP6981-Project-Iteration1/Project_Iteration_1.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m display(df_temp\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mengine_size\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mbody_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdescribe())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_drop' is not defined"
     ]
    }
   ],
   "source": [
    "df_temp = df_drop.copy()\n",
    "\n",
    "# engine_size per body_type category\n",
    "display(df_temp.groupby('engine_size')['body_type'].describe())\n",
    "\n",
    "\n",
    "# engine_size stats based on body_type of vehicles\n",
    "engine_size_stats = df_drop.groupby('body_type').agg({'engine_size' : ['mean', 'median', pd.Series.mode, 'std', 'min', 'max']})\n",
    "engine_size_stats = engine_size_stats[['std', 'min', 'mean', 'max', 'median', 'mode']]\n",
    "display(engine_size_stats)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(16,16))\n",
    "engine_size_stats.plot.barh(ax=axs)\n",
    "\n",
    "# all vehicles with 'Van' body_type are missing 'engine_size' attribute\n",
    "# display(df_drop[df_drop['body_type'] == 'Van']['engine_size'].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_drop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Users\\ojS\\Documents\\Family\\Miwo\\Graduate Studies\\MemorialU\\Pre-Adm. & Admission Documents\\Courses\\Terms\\Fall 2022\\COMP 6981 (DataPrepTech)\\proj\\COMP6981-Project-Iteration1\\Project_Iteration_1.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/ojS/Documents/Family/Miwo/Graduate%20Studies/MemorialU/Pre-Adm.%20%26%20Admission%20Documents/Courses/Terms/Fall%202022/COMP%206981%20%28DataPrepTech%29/proj/COMP6981-Project-Iteration1/Project_Iteration_1.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# save a record of rows that currently\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Users/ojS/Documents/Family/Miwo/Graduate%20Studies/MemorialU/Pre-Adm.%20%26%20Admission%20Documents/Courses/Terms/Fall%202022/COMP%206981%20%28DataPrepTech%29/proj/COMP6981-Project-Iteration1/Project_Iteration_1.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_temp \u001b[39m=\u001b[39m df_drop\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/ojS/Documents/Family/Miwo/Graduate%20Studies/MemorialU/Pre-Adm.%20%26%20Admission%20Documents/Courses/Terms/Fall%202022/COMP%206981%20%28DataPrepTech%29/proj/COMP6981-Project-Iteration1/Project_Iteration_1.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m display(df_temp[df_temp[\u001b[39m'\u001b[39m\u001b[39mengine_size\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misna()])                         \u001b[39m# rows with missing attr\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/ojS/Documents/Family/Miwo/Graduate%20Studies/MemorialU/Pre-Adm.%20%26%20Admission%20Documents/Courses/Terms/Fall%202022/COMP%206981%20%28DataPrepTech%29/proj/COMP6981-Project-Iteration1/Project_Iteration_1.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfix_engine_size\u001b[39m(val) :\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_drop' is not defined"
     ]
    }
   ],
   "source": [
    "# save a record of rows that are currently missing their engine_size\n",
    "# this way, we can compare our method of filling (using measures of centrality)\n",
    "#\n",
    "df_temp = df_drop.copy()\n",
    "display(df_temp[df_temp['engine_size'].isna()])                         # rows with missing attr\n",
    "\n",
    "def fix_engine_size(val) :\n",
    "    return engine_size_stats.loc[val, 'mean']\n",
    "\n",
    "df_temp['engine_size'].fillna(df_temp[df_temp['engine_size'].isna()]['body_type'].apply(fix_engine_size), inplace=True)\n",
    "display(df_temp[df_temp['engine_size'].isna()])                         # should be left with rows (vehicles) with no centrality values for their body_type (NaN for mean of Vans for instance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers\n",
    "For outliers, we decided to inspect all our numerical attributes, besides **year** and **age**:\n",
    "* **price**\n",
    "* **miles**\n",
    "* **engine_size**\n",
    "\n",
    "Let's get a preview of our outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(df, columns):\n",
    "    fig, axs = plt.subplots(len(columns),1,figsize=(16,18))\n",
    "    i = 0\n",
    "    for c in columns:\n",
    "        df[c].plot.box(ax=axs[i])\n",
    "        i+=1\n",
    "\n",
    "col_to_plot = ['price', 'miles', 'engine_size']\n",
    "plot_box(df_drop, col_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Cleanup\n",
    "\n",
    "Finally, we drop rows missing values from certain columns:\n",
    "* **vin** : fundamental attribute, cannot be retrieved from others\n",
    "* **miles** : cannot be retrieved from any other columns\n",
    "* **model** : is obfuscated in VIN (not a 1-to-1 mapping) since it varies across manufacturers (makes)\n",
    "* **body_type** : is also obfuscated in VIN (unique to each make); if missing for a particular model of a make, cannot be retrieved from other models\n",
    "* **city** : even if seller_name is available, if city is missing, it is missing in all instances of seller_name (available or also missing)  \n",
    "* **province** : same reason as city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a preview of the state of the dataset prior to dropping any rows\n",
    "print(f'State of the Dataset before dropping any rows...')\n",
    "display(df_drop.info())\n",
    "\n",
    "# we drop rows missing values from certain columns\n",
    "df_drop.dropna(subset=['miles', 'model','body_type', 'city', 'province'], inplace=True)\n",
    "\n",
    "# preview after dropping some rows based on above conditions\n",
    "print(f'State of the Dataset after dropping some rows...')\n",
    "display(df_drop.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-sampling to create working dataset\n",
    "\n",
    "Once we have dropped duplicate records, filled in missing data and taken care of outliers, it's finally time to select a sub-sample of our still dirty, but *slightly* cleaned, dataset.\n",
    "\n",
    "For this, we created a random sampler. Given the size of our initial dataset, and the size of the desired sample, it creates bins of roughly same sizes and randomly selects indices from each of these bins. These indices correspond to the indices of rows we will select from our main dataset, which we then proceed to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates and returns a random sample of indices [0...n-1] of length, size_sub_df\n",
    "def random_sampler(size_initial_df,  size_sub_df):\n",
    "    bin_size = round(size_initial_df/size_sub_df)\n",
    "    sample_indices = []\n",
    "    low = 0\n",
    "    high = 1\n",
    "\n",
    "    for i in range(0, size_sub_df+1):\n",
    "        sample_indices.append(np.random.randint((low*bin_size), (high*bin_size)))\n",
    "        low+=1\n",
    "        high+=1\n",
    "\n",
    "    return sample_indices\n",
    "\n",
    "# generate a list of 1500 random indices between 0 and n-1; n=len(df)\n",
    "sample_indices = random_sampler(len(df_drop), 1500)\n",
    "# display(sample_indices)\n",
    "\n",
    "# use generated indices to sample initial dataframe and create and subset\n",
    "df_sub = df_drop.iloc[sample_indices, :].copy()\n",
    "display(df_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Presentation\n",
    "\n",
    "Below are a few plots highlighting certain aspects of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because our data is not temporal, nor is it reasonable to plot against our index ('id'),\n",
    "# the natural progression is to plot the \n",
    "\n",
    "# plots non-normalized distribution\n",
    "def plot_all_series(df, columns):\n",
    "    fig, axs = plt.subplots(len(columns),1,figsize=(16,18))\n",
    "    axs = axs.flatten()\n",
    "    i = 0\n",
    "    for c in columns:\n",
    "        temp_ser = df.groupby(by=[c]).size()\n",
    "        axs[i].scatter(x=temp_ser.index, y=temp_ser, label=columns[i])\n",
    "        axs[i].legend(loc='upper center')\n",
    "        i+=1\n",
    "\n",
    "plot_all_series(df_sub, col_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead, we plot normalized probability density functions of our numerical attributes. We also utilize histograms as they better describe the congregation of certain values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms_density(df, columns):\n",
    "    fig, axs = plt.subplots(len(columns),1,figsize=(16,18))\n",
    "    i = 0\n",
    "    for c in columns:\n",
    "        df[c].hist(ax=axs[i], density=True)\n",
    "        df[c].plot.density(ax=axs[i], title=c)\n",
    "        i+=1\n",
    "\n",
    "col_to_plot = ['price', 'miles', 'year', 'engine_size']\n",
    "plot_histograms_density(df_sub, col_to_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afcc7df757c8b5b637700a66f7d6342733a2cc915737d4abb49e88b07ff3bb8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
