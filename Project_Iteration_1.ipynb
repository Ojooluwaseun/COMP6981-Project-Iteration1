{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Members\n",
    "\n",
    "**Ojo, Oluwaseun** | **Sholola, Oluwafunmiwo Judah**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context of Dataset and Goal Overview\n",
    "\n",
    "For our dataset, we settled on automotive data from MarketCheck spanning 8 years of inventory in Canada. Each record indicates the most recent online activity for a particular vehicle, obtained via webcrawlers that aggregate info retrieved from a vast number (~65k) of dealer websites. Online activity is not defined, but it is reasonably safe (based on the 'price' attribute provided) to assume that the records indicate sale listings (and not leases, or rentals).\n",
    "\n",
    "According to the dataset's [Kaggle link](https://www.kaggle.com/datasets/3ea0a6a45dbd4713a8759988845f1a58038036d84515ded58f65a2ff2bd32e00?resource=download), it was last updated a year ago, so we can reasonably expect 2021 to be the upper limit for the year of car manufacture.\n",
    "\n",
    "This dataset comprises some expected attributes such as the price, make, mileage and manufacturing year of the car. It also includes more specific (and potentially more interesting) attributes such as the fuel type, the engine size, the (corporate) dealer name and site of activity (zip and province).\n",
    "\n",
    "While our exact goal is not settled upon just yet, as we have not studied the data in-depth enough to hone in on a single phenom, listed below are a few possible phenomenon that are in contention to be studied by us in future iterations:\n",
    "\n",
    "* correlation of price to the make and year of a vehicle\n",
    "* correlation of vehicle fuel type to sales (or frequency of sales, more specifically)\n",
    "* the ideal car (or kind of car) to buy based on the age of vehicles\n",
    "* the ideal province to buy a used car within Canada taking price and mileage into consideration\n",
    "\n",
    "That said, let us dive in!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to importing our dataset, we quickly observe that the total number of records in the dataset and the number of unique VIN values **DO NOT** match. \n",
    "\n",
    "The reason for this disparity in the number of records and the VIN's is due to the fact that certain \n",
    "cars come up for sale multiple times within the collection window of the dataset (the past year), or they come multiple different dealer sites.\n",
    "\n",
    "<img src=\"img\\kaggle-id-vin.png\" width=\"600\" height=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('ca-dealers-used.csv')\n",
    "\n",
    "# confirming what we observe at a glance on Kaggle\n",
    "df['vin'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index selection\n",
    "\n",
    "Logically, it makes sense to use either the **id** attribute (which is unique) or the **vin** attribute (which, ideally, would be unique in the dataset).\n",
    "\n",
    "Since some VIN's pop up a few times in the dataset as a result of certain vehicles being traded multiple times, we have to decide:\n",
    "* if we want to drop duplicate VIN's and simply represent each vehicle once throughout the dataframe, thus making the **vin** attribute unique and possible for use as the **index** of the dataframe; *or*\n",
    "* if we want to keep multiple instances of a vehicle in the dataframe and utilize **id** as a single index, or **(id, vin)** as a MultiIndex **index** of the dataframe\n",
    "\n",
    "We decided to drop rows with duplicate VIN's as we do not require multiple instances of a vehicle for the purposes of our analysis. Subsequently, we set **id** attribute as our index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with duplicate VIN and set 'id' as our index\n",
    "df_copy = df.copy()\n",
    "df_copy = df_copy.drop_duplicates('vin')\n",
    "df_copy = df_copy.reset_index(drop=True).set_index('id')\n",
    "\n",
    "display(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a basic, high-level overview\n",
    "display(df_copy.info())\n",
    "\n",
    "# general description of numerical attributes in dataset\n",
    "display(df_copy.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for dropping certain columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision to drop these columns is moreso based on some domain knowledge and logic.\n",
    "\n",
    "The **stock_no** attribute is unique to each dealer's online inventory categorization system, and is not of a singular format; some are purely numeric in nature, while some are alphanumeric. As such, it is not extremely useful across diverse sources (dealers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first 20 'stock_no' fields\n",
    "display(df_copy['stock_no'].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to drop the **trim** attribute because of the sheer number of unique values it contains per **make**. It could lead to feature explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy.groupby('trim')['make'].describe())\n",
    "display(sum(df_copy.groupby('trim')['make'].describe()['unique']))\n",
    "\n",
    "display(df_copy.groupby('make')['trim'].describe())\n",
    "display(sum(df_copy.groupby('make')['trim'].describe()['unique']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **engine_block** is a low-level attribute that has **3** possible values:\n",
    "* H-engine type\n",
    "* I-engine type\n",
    "* V-engine type\n",
    "\n",
    "It is not a particularly desirable attribute for our purposes. \n",
    "\n",
    "Also, unlike some other attributes such as the manufacturing **year**, which can be decoded from the **VIN** and are uniform/standard across various vehicle manufacturers, the **engine_block** code in the VIN differs across manufacturers (**make**) and even across **models** within the same make. \n",
    "\n",
    "As such, it rapidly devolves into an arduous task in trying to retrieve an attribute that we, once again, do not view as useful to our goal(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy.groupby('engine_block').count())\n",
    "\n",
    "df_copy['engine_block'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **street** and **zip** attributes are being dropped in favour of 2 other geo-based attributes, **city** and **state**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proceeding to drop the unwanted columns and reordering our retained columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the columns and drop them\n",
    "col_to_drop = ['stock_no', 'trim', 'engine_block', 'street', 'zip']\n",
    "df_copy = df_copy.drop(columns=col_to_drop)\n",
    "\n",
    "# specify an order of columns and order them accordingly\n",
    "order_of_cols = ['vin', 'make', 'model', 'year','miles', 'price', 'fuel_type', 'engine_size', 'body_type',  'vehicle_type', 'transmission', 'drivetrain', 'seller_name', 'city', 'state']\n",
    "df_copy = df_copy[order_of_cols]\n",
    "\n",
    "# rename state to province (more appropriate to Canada)\n",
    "df_copy.rename(columns={'state' : 'province'}, inplace=True)\n",
    "\n",
    "df_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusion of an Additional (Derived) Column\n",
    "\n",
    "We decided to derive a new column, **age**, based off the manufacturing **year**, which we use to prune off vehicles that fall outside a specific range. It is important to note that this is not a trimming off of outliers, rather we are choosing deal *only* with cars that were manufactured *more recently*.\n",
    "\n",
    "This attribute might also come in handy in extracting useful features, such as the relationship between the manufacturing **year** of vehicles and the **frequency** (number) of sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['age'] = df_copy.year.apply(lambda x: 2022-x) # creating age column to know the age of the vehicles in our dataset\n",
    "print(f\"Ages Before\\nOldest: {df_copy['age'].max()}\\nNewest: {df_copy['age'].min()}\")\n",
    "display(df_copy.info())\n",
    "\n",
    "# \"\"\"We will plot a line plot to see the number of vehicles by age in order to know if we should keep or drop\n",
    "# rows based on the age of the vehicle\"\"\"\n",
    "\n",
    "#Plotting the numbers of vehicles by year\n",
    "df_copy['age'].value_counts().plot(kind='bar', figsize = (12, 6), title = 'Number of Vehicles by Age')\n",
    "plt.xlabel('Vehicle Age')\n",
    "plt.ylabel('Vehicle Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From our plot, we can observe that the vehicles that are newer have higher numbers than older vehicles.\n",
    "We will therefore drop rows with older cars. We will work with cars between ages 1 - 15\n",
    "\"\"\"\n",
    "\n",
    "df_copy = df_copy[(df_copy['age']>= 0) & (df_copy['age'] <= 15)] # removing vehicles that are older than 20 years old\n",
    "print(f\"Ages After\\nOldest: {df_copy['age'].max()}\\nNewest: {df_copy['age'].min()}\")\n",
    "display(df_copy.info())\n",
    "\n",
    "#Plotting the numbers of vehicles by year after removi g older cars\n",
    "df_copy['age'].value_counts().plot(kind='bar',figsize = (12, 6), title = 'Number of Vehicles by Age')\n",
    "plt.xlabel('Vehicle Age')\n",
    "plt.ylabel('Vehicle Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextualization and Description of Variables\n",
    "\n",
    "Our dataset is the collection of inventory of used cars from various auto dealers across Canada. The data depicts the market activity of used cars sold by different dealers in the various provinces of Canada. The following are the variables in the dataset and their description:\n",
    "\n",
    "**id** :    Unique identifier for each row.<br>\n",
    "            *Data type: string*<br>\n",
    "**vin** :   17 character long vehicle identification number. This is the value that uniquely identify each vehicle.<br> \n",
    "            *Data type: string* <br>\n",
    "**price** : Price of the vehicle. This is the value that we will try to predict.<br>\n",
    "            *Data type: float* <br>\n",
    "**miles** : Number of miles/odometer on the vehicle. The higher the number, the more the vehicle has been driven or used.<br>\n",
    "            *Data type: float* <br>\n",
    "**year** :  Model year of the vehicle - Decoded from VIN. This is the year the vehicle was manufactured. <br>\n",
    "            *Data type: float* <br>\n",
    "**make** :  Make of the vehicle - Decoded from VIN. This is the manufacturer of the vehicle. <br>\n",
    "            *Data type: string* <br>\n",
    "**model** : Model of the vehicle - Decoded from VIN. It is the specific model of the vehicle. <br>\n",
    "            *Data type: string* <br>\n",
    "**body_type** : Body type of the vehicle - Decoded from VIN. This is the type of vehicle. For example, sedan, coupe, SUV, etc. <br>\n",
    "            *Data type: string* <br>\n",
    "**vehicle_type** : This is the type of vehicle. For example, car, truck, etc. <br>\n",
    "            *Data type: string* <br>\n",
    "**engine_size** : This is the size of the engine in liters. <br>\n",
    "            *Data type: float*<br>\n",
    "**fuel_type** : This is the type of fuel the vehicle uses. For example, gasoline, diesel, etc. <br>\n",
    "            *Data type: string* <br>\n",
    "**transmission** : Transmission type of the vehicle. The transmission type is either automatic or manual. <br>\n",
    "            *Data type: string* <br>\n",
    "**drivetrain** : Drivetrain type of the vehicle. The drivetrain type is either 4WD, FWD, or RWD. <br>\n",
    "            *Data type: string* <br>\n",
    "**seller_name** : Name of the seller or dealership. <br>\n",
    "            *Data type: string* <br>\n",
    "**city** : City of the seller or dealership. <br>\n",
    "            *Data type: string* <br>\n",
    "**state** : State of the seller or dealership. <br>\n",
    "            *Data type: string* <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in Missing Data\n",
    "Two attributes we decided on, for addressing missing data are:\n",
    "* **year**; *and*\n",
    "* **engine_size**\n",
    "\n",
    "For **year**, it is **not** missing from the original dataset, so we are going to artificially introduce missing values in some rows. The intention (and result) is to repoduce these values by decoding the VIN. It is a standard across all manufacturers that the **10th digit** in the VIN is a *letter* or a *number*, which corresponds to the manufacturing year of the vehicle.\n",
    "\n",
    "<img src=\"img\\vin-year-dict-mapping.png\" width=\"600\" height=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 221742 entries, b39ea795-eca9 to 479607ed-62af\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   vin           221742 non-null  object \n",
      " 1   make          221742 non-null  object \n",
      " 2   model         219143 non-null  object \n",
      " 3   year          177394 non-null  float64\n",
      " 4   miles         209571 non-null  float64\n",
      " 5   price         200716 non-null  float64\n",
      " 6   fuel_type     178168 non-null  object \n",
      " 7   engine_size   177244 non-null  float64\n",
      " 8   body_type     201738 non-null  object \n",
      " 9   vehicle_type  199542 non-null  object \n",
      " 10  transmission  198074 non-null  object \n",
      " 11  drivetrain    196942 non-null  object \n",
      " 12  seller_name   220579 non-null  object \n",
      " 13  city          218043 non-null  object \n",
      " 14  province      217984 non-null  object \n",
      " 15  age           221742 non-null  float64\n",
      "dtypes: float64(5), object(11)\n",
      "memory usage: 36.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# function to introduce NaN into random rows under a particular column\n",
    "def add_NaN_random_rows(data, attribute, fraction):\n",
    "    for col in data.columns:\n",
    "        if col == attribute:\n",
    "            data.loc[data.sample(frac=fraction).index, col] = np.nan\n",
    "\n",
    "# place/introduce NaN into year for a random sample of 20% of our dataset,\n",
    "# effectively mimicking missing data under the 'year' attribute \n",
    "add_NaN_random_rows(df_copy, 'year', 0.2) \n",
    "df_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We are able to fill missing year from Vehicle VIN.\n",
    "This function generates year of the vehicle from the VIN.\n",
    "\"\"\"\n",
    "\n",
    "def generate_year(vin):\n",
    "    decoder = {'1':'2001', '2': '2002', '3': '2003', '4':'2004', '5':'2005', '6':'2006', '7':'2007', '8':'2008', '9':'2009', 'A':'2010', 'B':'2011', \n",
    "               'C':'2012', 'D':'2013', 'E':'2014', 'F':'2015', 'G':'2016', 'H':'2017', 'J':'2018', 'K':'2019', 'L':'2020', 'M':'2021', 'N':'2022'}\n",
    "    vin = str(vin)\n",
    "    if (len(vin) != 17) :\n",
    "        return\n",
    "    else:\n",
    "        year_code = str(vin)[-8]\n",
    "        \n",
    "        for key,value in decoder.items():\n",
    "            if year_code == key:\n",
    "                return value      \n",
    "    \n",
    "df_copy['year'] = df_copy['vin'].apply(generate_year)\n",
    "df_copy.sort_values(by=['year'], ascending=False)\n",
    "df_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **engine_size**, about **18%** of values are missing from this attribute in the original dataset. The engine size can be derived from the VIN, but the encoding is unique to each manufacturer and also to the particular model of a vehicle, so using a dictionary would be expensive (memory-wise). As such, we utilize centrality measures, based on categorization fo a related column (**body_type**), to fill missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine_size per body_type category\n",
    "display(df_copy.groupby('engine_size')['body_type'].describe())\n",
    "\n",
    "# engine_size stats based on body_type of vehicles\n",
    "engine_size_stats = df_copy.groupby('body_type')['engine_size'].agg({'mean', 'median', pd.Series.mode, 'std', 'min', 'max'})\n",
    "engine_size_stats = engine_size_stats[['std', 'min', 'mean', 'max', 'median', 'mode']]\n",
    "display(engine_size_stats)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(16,16))\n",
    "engine_size_stats.plot.barh(ax=axs)\n",
    "\n",
    "# all vehicles with 'Van' body_type are missing 'engine_size' attribute\n",
    "# display(df_copy[df_copy['body_type'] == 'Van']['engine_size'].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with NaN in body_type attr; can't derive a reasonable engine_size heuristic for them\n",
    "# this is really important;\n",
    "df_copy.dropna(subset=['body_type'], inplace=True)\n",
    "\n",
    "# we save a record of rows that are currently missing their engine_size (but do not have NaN as body_type (already dropped those above))\n",
    "# this way, we can compare our method of filling (using measures of centrality)\n",
    "# to some engine_size values derived using some model (if we do go down that route)\n",
    "engine_size_missing = df_copy[df_copy['engine_size'].isna()]\n",
    "display(engine_size_missing)   \n",
    "\n",
    "# returns some centrality measure (mean or median) of observed 'engine_sizes' for a vehicle of a certain 'body_type'\n",
    "def fix_engine_size(body_type) :\n",
    "    # print(engine_size_stats.loc[val, 'mean'])\n",
    "    return engine_size_stats.loc[body_type, 'mean']\n",
    "\n",
    "# replace missing engine_size with a specified mea\n",
    "df_copy['engine_size'].fillna(df_copy[df_copy['engine_size'].isna()]['body_type'].apply(fix_engine_size), inplace=True)\n",
    "\n",
    "# should be left with rows (vehicles) with no centrality values for their body_type (NaN for mean of Vans for instance)\n",
    "display(df_copy[df_copy['engine_size'].isna()])                         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers\n",
    "For outliers, we decided to inspect all our numerical attributes, besides **year** and **age**:\n",
    "* **price**\n",
    "* **miles**\n",
    "* **engine_size**\n",
    "\n",
    "Let's get a preview of our outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(df, columns):\n",
    "    fig, axs = plt.subplots(len(columns),1,figsize=(16,18))\n",
    "    i = 0\n",
    "    for c in columns:\n",
    "        df[c].plot.box(ax=axs[i])\n",
    "        i+=1\n",
    "\n",
    "col_to_plot = ['price', 'miles','engine_size']\n",
    "plot_box(df_copy, col_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove outliers\n",
    "def remove_outliers(data, cols):\n",
    "    for c in cols:\n",
    "        q25 = np.percentile(data[c], 25)\n",
    "        q75 = np.percentile(data[c], 75)\n",
    "        iqr = q75 - q25\n",
    "        lower_bound = q25 - (1.5 * iqr)\n",
    "        upper_bound = q75 + (1.5 * iqr)\n",
    "    \n",
    "    return data[(data[c] > lower_bound) & (data[c] < upper_bound)]\n",
    "\n",
    "col_with_outliers = col_to_plot   \n",
    "df_sub = remove_outliers(df_copy, col_with_outliers)\n",
    "df_sub.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Cleanup\n",
    "\n",
    "Finally, we drop rows missing values from certain columns:\n",
    "* **vin** : fundamental attribute, cannot be retrieved from others\n",
    "* **miles** : cannot be retrieved from any other columns\n",
    "* **model** : is obfuscated in VIN (not a 1-to-1 mapping) since it varies across manufacturers (makes)\n",
    "* **body_type** : is also obfuscated in VIN (unique to each make); if missing for a particular model of a make, cannot be retrieved from other models\n",
    "* **city** : even if seller_name is available, if city is missing, it is missing in all instances of seller_name (available or also missing)  \n",
    "* **province** : same reason as city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a preview of the state of the dataset prior to dropping any rows\n",
    "print(f'State of the Dataset before dropping any rows...')\n",
    "display(df_copy.info())\n",
    "\n",
    "# we drop rows missing values from certain columns\n",
    "df_copy.dropna(inplace=True)\n",
    "\n",
    "# preview after dropping some rows based on above conditions\n",
    "print(f'State of the Dataset after dropping some rows...')\n",
    "display(df_copy.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-sampling to create working dataset\n",
    "\n",
    "Once we have dropped duplicate records, filled in missing data and taken care of outliers, it's finally time to select a sub-sample of our still dirty, but *slightly* cleaned, dataset.\n",
    "\n",
    "For this, we created a random sampler. Given the size of our initial dataset, and the size of the desired sample, it creates bins of roughly same sizes and randomly selects indices from each of these bins. These indices correspond to the indices of rows we will select from our main dataset, which we then proceed to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates and returns a random sample of indices [0...n-1] of length, size_sub_df\n",
    "def random_sampler(size_initial_df,  size_sub_df):\n",
    "    bin_size = round(size_initial_df/size_sub_df)\n",
    "    sample_indices = []\n",
    "    low = 0\n",
    "    high = 1\n",
    "\n",
    "    for i in range(0, size_sub_df+1):\n",
    "        sample_indices.append(np.random.randint((low*bin_size), (high*bin_size)))\n",
    "        low+=1\n",
    "        high+=1\n",
    "\n",
    "    return sample_indices\n",
    "\n",
    "# generate a list of 1500 random indices between 0 and n-1; n=len(df)\n",
    "sample_indices = random_sampler(len(df_copy), 1500)\n",
    "# display(sample_indices)\n",
    "\n",
    "# use generated indices to sample initial dataframe and create and subset\n",
    "df_sub = df_copy.iloc[sample_indices, :].copy()\n",
    "df_sub.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Presentation\n",
    "\n",
    "Since our data is not temporal, we plotted each numerical attribute against its frequency. Below are the non-normalized probability distribution functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because our data is not temporal, nor is it reasonable to plot against our index ('id'),\n",
    "# the natural progression is to plot the \n",
    "\n",
    "# plots non-normalized distribution\n",
    "def plot_all_series(df, columns):\n",
    "    fig, axs = plt.subplots(len(columns),1,figsize=(16,18))\n",
    "    axs = axs.flatten()\n",
    "    i = 0\n",
    "    for c in columns:\n",
    "        temp_ser = df.groupby(by=[c]).size()\n",
    "        axs[i].scatter(x=temp_ser.index, y=temp_ser, label=columns[i])\n",
    "        axs[i].legend(loc='upper center')\n",
    "        i+=1\n",
    "\n",
    "plot_all_series(df_sub, col_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead, we plot normalized probability density functions of our numerical attributes. We also utilize histograms as they better describe the congregation of certain values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms_density(df, columns):\n",
    "    fig, axs = plt.subplots(len(columns),1,figsize=(16,18))\n",
    "    i = 0\n",
    "    for c in columns:\n",
    "        df[c].hist(ax=axs[i], density=True)\n",
    "        df[c].plot.density(ax=axs[i], title=c)\n",
    "        i+=1\n",
    "\n",
    "col_to_plot = ['price', 'miles', 'age', 'engine_size']\n",
    "plot_histograms_density(df_sub, col_to_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01a38c6c4ccba8cef90af14012931c96cc6cad1377f3531fb1c2b3ab481a77ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
